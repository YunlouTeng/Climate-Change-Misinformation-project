{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3911a094-8aa5-4ad9-b24a-00fa2046e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "\n",
    "# NLTK Stop words\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "#remove urls, mentions, emojis \n",
    "import preprocessor as p\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01936baf-d833-49d0-9aca-11cfeceac718",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Users/lavine/Desktop/Terriers!/misinformation project/data/data.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cdfa55-4950-4097-b99c-095be9c758aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Create a dataframe for the first dictionary (failed right now)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847093d-b512-4352-9f63-13591f6fd5d5",
   "metadata": {},
   "source": [
    "Take a look at our dirst dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9707b3c-2448-4c8d-90e1-10a6f7efc2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_json = pd.read_json(filepath, lines=True, nrows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8144534b-f5eb-4593-86e2-07c61e4ba82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    und\n",
       "Name: lang, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_0_json['lang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17684ebf-2151-4a6d-9025-5a2ef269d760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>id</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>referenced_tweets</th>\n",
       "      <th>created_at</th>\n",
       "      <th>source</th>\n",
       "      <th>entities</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>reply_settings</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>public_metrics</th>\n",
       "      <th>author</th>\n",
       "      <th>in_reply_to_user</th>\n",
       "      <th>__twarc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>und</td>\n",
       "      <td>1477065589361315840</td>\n",
       "      <td>False</td>\n",
       "      <td>1460383957262880768</td>\n",
       "      <td>@tjbabel @Catherine_LCP @SamBoik @Cernovich ht...</td>\n",
       "      <td>[{'type': 'replied_to', 'id': '147706468563978...</td>\n",
       "      <td>2021-12-31 23:54:13+00:00</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>{'urls': [{'start': 44, 'end': 67, 'url': 'htt...</td>\n",
       "      <td>1733391854</td>\n",
       "      <td>everyone</td>\n",
       "      <td>1476711967607853056</td>\n",
       "      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>{'verified': False, 'created_at': '2021-11-15T...</td>\n",
       "      <td>{'verified': False, 'created_at': '2013-09-05T...</td>\n",
       "      <td>{'url': 'https://api.twitter.com/2/tweets/sear...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                   id  possibly_sensitive            author_id  \\\n",
       "0  und  1477065589361315840               False  1460383957262880768   \n",
       "\n",
       "                                                text  \\\n",
       "0  @tjbabel @Catherine_LCP @SamBoik @Cernovich ht...   \n",
       "\n",
       "                                   referenced_tweets  \\\n",
       "0  [{'type': 'replied_to', 'id': '147706468563978...   \n",
       "\n",
       "                 created_at              source  \\\n",
       "0 2021-12-31 23:54:13+00:00  Twitter for iPhone   \n",
       "\n",
       "                                            entities  in_reply_to_user_id  \\\n",
       "0  {'urls': [{'start': 44, 'end': 67, 'url': 'htt...           1733391854   \n",
       "\n",
       "  reply_settings      conversation_id  \\\n",
       "0       everyone  1476711967607853056   \n",
       "\n",
       "                                      public_metrics  \\\n",
       "0  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
       "\n",
       "                                              author  \\\n",
       "0  {'verified': False, 'created_at': '2021-11-15T...   \n",
       "\n",
       "                                    in_reply_to_user  \\\n",
       "0  {'verified': False, 'created_at': '2013-09-05T...   \n",
       "\n",
       "                                             __twarc  \n",
       "0  {'url': 'https://api.twitter.com/2/tweets/sear...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_0_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5aa15e-14fe-4d05-9462-3e5b84aed595",
   "metadata": {
    "tags": []
   },
   "source": [
    " ### Try to Create a dataframe by every columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1d9ce5af-9975-4db1-bf43-64d4a2f34184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all the JSON objects in one list as dictionaries\n",
    "data = []\n",
    "with open(filepath,'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ffb3d4f-05dc-421e-865a-f671418b3e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\'We are in a climate emergency\\'December wildfires ravage Colorado https://t.co/sWDkZM4Rvn Sen. Manchin, your Exxon highway bill isn\\'t going 2save our homes or our lives,\" \"Your greed &amp; corruption is not only torching our future It\\'s burning our communities and destroying lives!'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[9]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad5915a-4c51-48bd-8bb0-3d34265251f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461a236c-c225-47da-8eeb-99d6b224e268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "047709f8-ac5a-4db6-8faa-aad6ecb46d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@ScottGrimnes @MaMan564 @SoFarFletched ‘We’ started calling it ‘climate change’ when Exxon Mobile did a very successful PR campaign in the 90’s. Apparently ‘global warming’ is too specific and scary. But It’s global warming. https://t.co/WPetiWDQG3'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[4]['referenced_tweets'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a17e1-b7bd-41f8-852d-3260c48784ee",
   "metadata": {},
   "source": [
    " #### Put the text data in one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b7edffb1-c0bd-44d0-bdb6-93bb3318a2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrive text and avoid truncated text when retweet without quoting\n",
    "lst_text = []\n",
    "for i in data:    \n",
    "    if 'referenced_tweets' in i:\n",
    "        if i['referenced_tweets'][0]['type'] == 'retweeted':\n",
    "            lst_text.append(i['referenced_tweets'][0]['text'])\n",
    "        else:\n",
    "            lst_text.append(i['text'])\n",
    "    else:\n",
    "        lst_text.append(i['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6d5fe5f6-509e-421f-96d0-1a1dd3ad327c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@tjbabel @Catherine_LCP @SamBoik @Cernovich https://t.co/a5p9XOE7b9',\n",
       " '@tjbabel @Catherine_LCP @SamBoik @Cernovich https://t.co/BRBvadbSVr']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_text[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8768c389-a75e-45af-be8b-cf8940199c75",
   "metadata": {},
   "source": [
    " #### Put the author info in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3ad13fe-db12-4fe4-a859-87dd1586bebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lang', 'id', 'possibly_sensitive', 'author_id', 'text', 'referenced_tweets', 'created_at', 'source', 'entities', 'in_reply_to_user_id', 'reply_settings', 'conversation_id', 'public_metrics', 'author', 'in_reply_to_user', '__twarc'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70882ac5-c59d-4020-b77f-dae8dc32b489",
   "metadata": {},
   "source": [
    " ##### 'id', 'author_id', 'conversation_id', 'author'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "98da3718-a811-41dc-b879-4b862fd2cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists for id, author id and conversation\n",
    "lst_id = []\n",
    "lst_lang = []\n",
    "lst_author_id = []\n",
    "lst_conversation_id = []\n",
    "for i in data:\n",
    "    lst_id.append(i['id'])\n",
    "    lst_lang.append(i['lang'])\n",
    "    lst_author_id.append(i['author_id'])\n",
    "    lst_conversation_id.append(i['conversation_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a95d19-4d5a-49ca-9f34-d4fd0a8c2fda",
   "metadata": {},
   "source": [
    "There are too much info in author dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5f03dbf4-4956-44a2-911a-361deabe3cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verified': False,\n",
       " 'created_at': '2021-11-15T23:07:29.000Z',\n",
       " 'public_metrics': {'followers_count': 12,\n",
       "  'following_count': 245,\n",
       "  'tweet_count': 963,\n",
       "  'listed_count': 0},\n",
       " 'name': 'Owen Lavine',\n",
       " 'description': 'Journalist - Cal Poly SLO ‘24 - LA',\n",
       " 'protected': False,\n",
       " 'username': 'owen_lavine',\n",
       " 'url': '',\n",
       " 'profile_image_url': 'https://pbs.twimg.com/profile_images/1460384644122099713/ToEk8Ndb_normal.jpg',\n",
       " 'id': '1460383957262880776',\n",
       " 'pinned_tweet_id': '1482061096068341760'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d8402d33-10ed-4271-a81e-c53ead6b3117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lst verified or not, author name and author username\n",
    "lst_author_verified = []\n",
    "lst_author_name = []\n",
    "lst_author_username = []\n",
    "for i in data:\n",
    "    lst_author_verified.append(i['author']['verified'])\n",
    "    lst_author_name.append(i['author']['name'])\n",
    "    lst_author_username.append(i['author']['username'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dc9a84-57a0-4a0c-b312-945a46893063",
   "metadata": {},
   "source": [
    " ##### Make creation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "abaac408-58a0-4fab-a4e7-1e1cd6713fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list creation time\n",
    "lst_twt_creation_time = []\n",
    "for i in data:\n",
    "    lst_twt_creation_time.append(i['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a61f6d-633b-4c1d-a9ea-0599dfeb3afd",
   "metadata": {},
   "source": [
    "##### Put what we have in one dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27356dd-560c-4c38-833e-1ee09bc44f64",
   "metadata": {},
   "source": [
    "lst_text,lst_id, lst_author_id,lst_conversation_id,lst_author_verified, lst_author_name, lst_author_username,lst_twt_creation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3b0e8f52-6dfe-4a2b-bbb4-c5ec3fefb98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame({'text': lst_text,\n",
    "                  'language': lst_lang,\n",
    "                  'id': lst_id,\n",
    "                  'author_id': lst_author_id,\n",
    "                  'conversation_id': lst_conversation_id,\n",
    "                  'author_verified': lst_author_verified,\n",
    "                  'author_name': lst_author_name,\n",
    "                  'author_username': lst_author_username,\n",
    "                  'twt_creation_time': lst_twt_creation_time\n",
    "                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "188315ed-e472-403c-9860-0cd9bebccca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5e7c9a2a-cb70-427a-900a-44f7c76611c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.drop(tweets[tweets['language'] != 'en'].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "165a90ad-d8bd-47dd-bf62-c30c6cfd4a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>author_verified</th>\n",
       "      <th>author_name</th>\n",
       "      <th>author_username</th>\n",
       "      <th>twt_creation_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm thinking about writing a satire where the U.S. Congress is blocked from taking action on the climate by a Senator who owns a coal company, lives on a yacht and speaks daily with Exxon lobbyists -- but I'm worried people will think it's too on the nose. 😔</td>\n",
       "      <td>en</td>\n",
       "      <td>1477059914015125507</td>\n",
       "      <td>2732778706</td>\n",
       "      <td>1477059914015125507</td>\n",
       "      <td>False</td>\n",
       "      <td>Tai Chi</td>\n",
       "      <td>TaiWendysb</td>\n",
       "      <td>2021-12-31T23:31:40.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Mum2Mrs @oshima9 no, capitalists are the cause.\\nhttps://t.co/VyFfKhY2Yp</td>\n",
       "      <td>en</td>\n",
       "      <td>1477056800889323523</td>\n",
       "      <td>414239235</td>\n",
       "      <td>1477056800889323523</td>\n",
       "      <td>False</td>\n",
       "      <td>Richard Estes</td>\n",
       "      <td>oshima9</td>\n",
       "      <td>2021-12-31T23:19:18.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@ScottGrimnes @MaMan564 @SoFarFletched ‘We’ started calling it ‘climate change’ when Exxon Mobile did a very successful PR campaign in the 90’s. Apparently ‘global warming’ is too specific and scary. But It’s global warming. https://t.co/WPetiWDQG3</td>\n",
       "      <td>en</td>\n",
       "      <td>1477046502400868359</td>\n",
       "      <td>824946672673361920</td>\n",
       "      <td>1477046502400868359</td>\n",
       "      <td>False</td>\n",
       "      <td>Kris</td>\n",
       "      <td>writing_callous</td>\n",
       "      <td>2021-12-31T22:38:23.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I'm thinking about writing a satire where the U.S. Congress is blocked from taking action on the climate by a Senator who owns a coal company, lives on a yacht and speaks daily with Exxon lobbyists -- but I'm worried people will think it's too on the nose. 😔</td>\n",
       "      <td>en</td>\n",
       "      <td>1477040827503362057</td>\n",
       "      <td>2429328178</td>\n",
       "      <td>1477040827503362057</td>\n",
       "      <td>False</td>\n",
       "      <td>David💙l🇪🇺#FindMissyandBiscuit</td>\n",
       "      <td>David96212152</td>\n",
       "      <td>2021-12-31T22:15:50.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@Mum2Mrs @oshima9 no, capitalists are the cause.\\nhttps://t.co/VyFfKhY2Yp</td>\n",
       "      <td>en</td>\n",
       "      <td>1477035647462363141</td>\n",
       "      <td>795750344970141701</td>\n",
       "      <td>1476641658636693513</td>\n",
       "      <td>False</td>\n",
       "      <td>☭🇵🇸🇾🇪🇸🇾</td>\n",
       "      <td>zia_kat</td>\n",
       "      <td>2021-12-31T21:55:15.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                 text  \\\n",
       "2  I'm thinking about writing a satire where the U.S. Congress is blocked from taking action on the climate by a Senator who owns a coal company, lives on a yacht and speaks daily with Exxon lobbyists -- but I'm worried people will think it's too on the nose. 😔   \n",
       "3                                                                                                                                                                                           @Mum2Mrs @oshima9 no, capitalists are the cause.\\nhttps://t.co/VyFfKhY2Yp   \n",
       "4            @ScottGrimnes @MaMan564 @SoFarFletched ‘We’ started calling it ‘climate change’ when Exxon Mobile did a very successful PR campaign in the 90’s. Apparently ‘global warming’ is too specific and scary. But It’s global warming. https://t.co/WPetiWDQG3   \n",
       "5  I'm thinking about writing a satire where the U.S. Congress is blocked from taking action on the climate by a Senator who owns a coal company, lives on a yacht and speaks daily with Exxon lobbyists -- but I'm worried people will think it's too on the nose. 😔   \n",
       "6                                                                                                                                                                                           @Mum2Mrs @oshima9 no, capitalists are the cause.\\nhttps://t.co/VyFfKhY2Yp   \n",
       "\n",
       "  language                   id           author_id      conversation_id  \\\n",
       "2       en  1477059914015125507          2732778706  1477059914015125507   \n",
       "3       en  1477056800889323523           414239235  1477056800889323523   \n",
       "4       en  1477046502400868359  824946672673361920  1477046502400868359   \n",
       "5       en  1477040827503362057          2429328178  1477040827503362057   \n",
       "6       en  1477035647462363141  795750344970141701  1476641658636693513   \n",
       "\n",
       "   author_verified                    author_name  author_username  \\\n",
       "2            False                        Tai Chi       TaiWendysb   \n",
       "3            False                  Richard Estes          oshima9   \n",
       "4            False                           Kris  writing_callous   \n",
       "5            False  David💙l🇪🇺#FindMissyandBiscuit    David96212152   \n",
       "6            False                        ☭🇵🇸🇾🇪🇸🇾          zia_kat   \n",
       "\n",
       "          twt_creation_time  \n",
       "2  2021-12-31T23:31:40.000Z  \n",
       "3  2021-12-31T23:19:18.000Z  \n",
       "4  2021-12-31T22:38:23.000Z  \n",
       "5  2021-12-31T22:15:50.000Z  \n",
       "6  2021-12-31T21:55:15.000Z  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f8239d-0094-46e7-ab51-c559cb9963fa",
   "metadata": {},
   "source": [
    "### data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7ae12cec-2bd4-43dc-bdd8-e442a6907d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['hashtag'] = tweets['text'].apply(lambda x: re.findall(r\"#(\\w+)\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8d4bb11c-5b98-452c-8ea9-e1f0f4e36ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forming a separate feature for cleaned tweets\n",
    "tweets['ctext'] = tweets['text'].apply(lambda x: p.clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9b9e878d-4990-4b58-85f2-2e42650a652f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>author_verified</th>\n",
       "      <th>author_name</th>\n",
       "      <th>author_username</th>\n",
       "      <th>twt_creation_time</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>ctext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm thinking about writing a satire where the U.S. Congress is blocked from taking action on the climate by a Senator who owns a coal company, lives on a yacht and speaks daily with Exxon lobbyists -- but I'm worried people will think it's too on the nose. 😔</td>\n",
       "      <td>en</td>\n",
       "      <td>1477059914015125507</td>\n",
       "      <td>2732778706</td>\n",
       "      <td>1477059914015125507</td>\n",
       "      <td>False</td>\n",
       "      <td>Tai Chi</td>\n",
       "      <td>TaiWendysb</td>\n",
       "      <td>2021-12-31T23:31:40.000Z</td>\n",
       "      <td>[]</td>\n",
       "      <td>I'm thinking about writing a satire where the U.S. Congress is blocked from taking action on the climate by a Senator who owns a coal company, lives on a yacht and speaks daily with Exxon lobbyists -- but I'm worried people will think it's too on the nose.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Mum2Mrs @oshima9 no, capitalists are the cause.\\nhttps://t.co/VyFfKhY2Yp</td>\n",
       "      <td>en</td>\n",
       "      <td>1477056800889323523</td>\n",
       "      <td>414239235</td>\n",
       "      <td>1477056800889323523</td>\n",
       "      <td>False</td>\n",
       "      <td>Richard Estes</td>\n",
       "      <td>oshima9</td>\n",
       "      <td>2021-12-31T23:19:18.000Z</td>\n",
       "      <td>[]</td>\n",
       "      <td>no, capitalists are the cause.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@ScottGrimnes @MaMan564 @SoFarFletched ‘We’ started calling it ‘climate change’ when Exxon Mobile did a very successful PR campaign in the 90’s. Apparently ‘global warming’ is too specific and scary. But It’s global warming. https://t.co/WPetiWDQG3</td>\n",
       "      <td>en</td>\n",
       "      <td>1477046502400868359</td>\n",
       "      <td>824946672673361920</td>\n",
       "      <td>1477046502400868359</td>\n",
       "      <td>False</td>\n",
       "      <td>Kris</td>\n",
       "      <td>writing_callous</td>\n",
       "      <td>2021-12-31T22:38:23.000Z</td>\n",
       "      <td>[]</td>\n",
       "      <td>We started calling it climate change when Exxon Mobile did a very successful PR campaign in the s. Apparently global warming is too specific and scary. But Its global warming.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                 text  \\\n",
       "2  I'm thinking about writing a satire where the U.S. Congress is blocked from taking action on the climate by a Senator who owns a coal company, lives on a yacht and speaks daily with Exxon lobbyists -- but I'm worried people will think it's too on the nose. 😔   \n",
       "3                                                                                                                                                                                           @Mum2Mrs @oshima9 no, capitalists are the cause.\\nhttps://t.co/VyFfKhY2Yp   \n",
       "4            @ScottGrimnes @MaMan564 @SoFarFletched ‘We’ started calling it ‘climate change’ when Exxon Mobile did a very successful PR campaign in the 90’s. Apparently ‘global warming’ is too specific and scary. But It’s global warming. https://t.co/WPetiWDQG3   \n",
       "\n",
       "  language                   id           author_id      conversation_id  \\\n",
       "2       en  1477059914015125507          2732778706  1477059914015125507   \n",
       "3       en  1477056800889323523           414239235  1477056800889323523   \n",
       "4       en  1477046502400868359  824946672673361920  1477046502400868359   \n",
       "\n",
       "   author_verified    author_name  author_username         twt_creation_time  \\\n",
       "2            False        Tai Chi       TaiWendysb  2021-12-31T23:31:40.000Z   \n",
       "3            False  Richard Estes          oshima9  2021-12-31T23:19:18.000Z   \n",
       "4            False           Kris  writing_callous  2021-12-31T22:38:23.000Z   \n",
       "\n",
       "  hashtag  \\\n",
       "2      []   \n",
       "3      []   \n",
       "4      []   \n",
       "\n",
       "                                                                                                                                                                                                                                                              ctext  \n",
       "2  I'm thinking about writing a satire where the U.S. Congress is blocked from taking action on the climate by a Senator who owns a coal company, lives on a yacht and speaks daily with Exxon lobbyists -- but I'm worried people will think it's too on the nose.  \n",
       "3                                                                                                                                                                                                                                    no, capitalists are the cause.  \n",
       "4                                                                                   We started calling it climate change when Exxon Mobile did a very successful PR campaign in the s. Apparently global warming is too specific and scary. But Its global warming.  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e458abf3-644f-47bd-b8c6-43f7d4491501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc558d-1a18-4d40-8840-b0638ed6d41a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bef3a4db-2aa1-412a-b4b6-6a07696d38d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to list\n",
    "ctext = tweets['ctext'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "25ad07de-a943-4a3d-a64e-0a555ecc16d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove new line characters\n",
    "ctext = [re.sub('\\s+', ' ', sent) for sent in ctext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "10de78ad-cd53-4a1e-b4d8-42cf0466e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove distracting single quotes\n",
    "ctext = [re.sub(\"\\'\", \"\", sent) for sent in ctext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a06b12d3-e30d-437a-82d6-8ff049bc9a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "ctext = [re.sub(r'[^\\w\\s]', '', sent) for sent in ctext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e48ea990-81ac-4293-8fc2-cad5bc777db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no capitalists are the cause',\n",
       " 'We started calling it climate change when Exxon Mobile did a very successful PR campaign in the s Apparently global warming is too specific and scary But Its global warming',\n",
       " 'Im thinking about writing a satire where the US Congress is blocked from taking action on the climate by a Senator who owns a coal company lives on a yacht and speaks daily with Exxon lobbyists  but Im worried people will think its too on the nose',\n",
       " 'no capitalists are the cause']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctext[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee9d1b7-d916-49e8-9467-c99192da368b",
   "metadata": {},
   "source": [
    "### Tokenize words and further clean up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a3bd80-1498-427b-94a2-0d0cd9273d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize each sentence into a list of words, removing punctuations and unnecessary characters altoghter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6b44245f-17f9-45d5-8ebd-bc0c18503060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['im', 'thinking', 'about', 'writing', 'satire', 'where', 'the', 'us', 'congress', 'is', 'blocked', 'from', 'taking', 'action', 'on', 'the', 'climate', 'by', 'senator', 'who', 'owns', 'coal', 'company', 'lives', 'on', 'yacht', 'and', 'speaks', 'daily', 'with', 'exxon', 'lobbyists', 'but', 'im', 'worried', 'people', 'will', 'think', 'its', 'too', 'on', 'the', 'nose']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "tweet_words = list(sent_to_words(ctext))\n",
    "\n",
    "print(tweet_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8bef03c9-848f-4004-824e-b2340c6086f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['im', 'thinking', 'about_writing_satire', 'where', 'the', 'us', 'congress', 'is', 'blocked', 'from', 'taking', 'action', 'on', 'the', 'climate', 'by', 'senator', 'who', 'owns', 'coal', 'company', 'lives', 'on', 'yacht', 'and', 'speaks', 'daily', 'with', 'exxon', 'lobbyists', 'but', 'im', 'worried', 'people', 'will', 'think', 'its', 'too', 'on', 'the', 'nose']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(tweet_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[tweet_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[tweet_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a6586-a208-47dc-b7df-e84b8c5a621a",
   "metadata": {},
   "source": [
    "### Remove Stopwords, Make Bigrams and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b527df8e-7e82-45eb-995f-64d60d7d5dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3a9ae696-61af-40ea-8380-ea5b22393074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "91edbe4c-1e69-4d5c-b09a-318a5c3047d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "246f1398-6c05-4437-9052-fdc9f25af0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "tweet_words_nostops = remove_stopwords(tweet_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "29c6dc85-e89e-4ff4-a4d1-83fede7b1a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "tweet_words_bigrams = make_bigrams(tweet_words_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ccc2cc77-8a72-423b-ae6c-6546c8ade7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['m', 'think', 'block', 'take', 'action', 'climate', 'senator', 'own', 'coal', 'company', 'live', 'yacht', 'speak', 'daily', 'lobbyist', 'm', 'worried', 'people', 'think', 'nose']]\n"
     ]
    }
   ],
   "source": [
    "# Initialize spacy 'en_core_web_sm' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])# not \"en\" any more\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "tweet_lemmatized = lemmatization(tweet_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(tweet_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b9a70-7bf6-4b5b-8391-76185dd7cea7",
   "metadata": {},
   "source": [
    "### creat the dictionary and the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "045af5c1-03ce-4c46-a4ce-5d2510470d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 2), (16, 1), (17, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(tweet_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = tweet_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts] ##### ?????\n",
    "\n",
    "#Gensim creates a unique id for each word in the document. \n",
    "#The produced corpus shown below is a mapping of (word_id, word_frequency(times of occurance)).\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c70fb-e5f5-497a-89f3-9892a245c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7c2ed933-af4b-42eb-a1f3-d6e64196b7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'company'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If you want to see what word a given id corresponds to, pass the id as a key to the dictionary.\n",
    "id2word[4]\n",
    "\n",
    "#in put of LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5520aec4-4caa-4db2-bcad-f2eea66c6b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('action', 1),\n",
       "  ('block', 1),\n",
       "  ('climate', 1),\n",
       "  ('coal', 1),\n",
       "  ('company', 1),\n",
       "  ('daily', 1),\n",
       "  ('live', 1),\n",
       "  ('lobbyist', 1),\n",
       "  ('m', 2),\n",
       "  ('nose', 1),\n",
       "  ('own', 1),\n",
       "  ('people', 1),\n",
       "  ('senator', 1),\n",
       "  ('speak', 1),\n",
       "  ('take', 1),\n",
       "  ('think', 2),\n",
       "  ('worried', 1),\n",
       "  ('yacht', 1)]]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82855c8f-5efb-4c9d-872f-6a1ac7954577",
   "metadata": {},
   "source": [
    "### Building the Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fd2479a5-4314-45dd-98d5-a06ef929f664",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kg/0ndh6pln2x7b6rpm16twhvpc0000gn/T/ipykernel_61149/2777500870.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build LDA model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                            \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                            \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                            \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m             self.add_lifecycle_event(\n\u001b[1;32m    522\u001b[0m                 \u001b[0;34m\"created\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reached the end of input; now waiting for all remaining jobs to finish\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m                         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass_\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m                     \u001b[0;32mdel\u001b[0m \u001b[0mother\u001b[0m  \u001b[0;31m# frees up memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_mstep\u001b[0;34m(self, rho, other, extra_pass)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m         \u001b[0mcurrent_Elogbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_Elogbeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_Elogbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mget_Elogbeta\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mPosterior\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \"\"\"\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=30, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c1416a-9b9f-49b1-95b7-4105946d733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a08cd6-7d24-4d44-b4b9-398195a86847",
   "metadata": {},
   "source": [
    "### compute model perplexity and coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d82ce0a-a3f3-410d-84e0-140df9173b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77942d7d-c8f8-4bd2-9df3-2fd7cdbb7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f4e9e-9158-4afc-933d-8571b63db883",
   "metadata": {},
   "source": [
    "### find the optimal number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad00526-2503-4f3f-9f45-cbf49d3eb72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3900923d-71e3-48cc-9874-a5c45678b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3415d926-fdd9-4d0e-8d5e-7620b147d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa752331-e3ad-4d55-8b56-b4b5b2be33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
