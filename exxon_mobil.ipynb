{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8297f9dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3911a094-8aa5-4ad9-b24a-00fa2046e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "\n",
    "# NLTK Stop words\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "#remove urls, mentions, emojis \n",
    "import preprocessor as p\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01936baf-d833-49d0-9aca-11cfeceac718",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Users/lavine/Desktop/Terriers!/misinformation project/data/data.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5aa15e-14fe-4d05-9462-3e5b84aed595",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a dataframe by every columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ce5af-9975-4db1-bf43-64d4a2f34184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all the JSON objects in one list as dictionaries\n",
    "data = []\n",
    "with open(filepath,'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a17e1-b7bd-41f8-852d-3260c48784ee",
   "metadata": {},
   "source": [
    " #### Put the text data in one list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ba2fbf-e5ec-464c-8b6c-8f755bd24224",
   "metadata": {},
   "source": [
    "not original:\n",
    "referenced_tweet in i:\n",
    " == retweet \n",
    " \n",
    "lst_\n",
    "\n",
    "\n",
    "original:\n",
    "if not 'referenced_tweets' in i\n",
    "\n",
    "lst_o_text.append(i['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae72b75-2e31-4eeb-8207-5db49fa05477",
   "metadata": {},
   "source": [
    "#retrive text and avoid truncated text when retweet without quoting\n",
    "lst_o_text = []\n",
    "lst_ret_text = []\n",
    "lst_rep_text = []\n",
    "\n",
    "for i in data:    \n",
    "    if 'referenced_tweets' in i:\n",
    "        if i['referenced_tweets'][0]['type'] != 'retweeted':\n",
    "            lst_text.append(i['referenced_tweets'][0]['text'])\n",
    "        else:\n",
    "            lst_text.append(i['text'])\n",
    "    else:\n",
    "        lst_text.append(i['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5f8967-f8d6-4a4f-a72b-a71ba0eaba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_o_text = []\n",
    "\n",
    "#create lists for id, author id and conversation\n",
    "lst_id = []\n",
    "lst_lang = []\n",
    "lst_author_id = []\n",
    "lst_conversation_id = []\n",
    "\n",
    "#lst verified or not, author name and author username\n",
    "lst_author_verified = []\n",
    "lst_author_name = []\n",
    "lst_author_username = []\n",
    "\n",
    "#list creation time\n",
    "lst_twt_creation_time = []\n",
    "\n",
    "for i in data:\n",
    "    if not 'referenced_tweets' in i:\n",
    "        lst_o_text.append(i['text'])\n",
    "        lst_id.append(i['id'])\n",
    "        lst_lang.append(i['lang'])\n",
    "        lst_author_id.append(i['author_id'])\n",
    "        lst_conversation_id.append(i['conversation_id'])\n",
    "        lst_author_verified.append(i['author']['verified'])\n",
    "        lst_author_name.append(i['author']['name'])\n",
    "        lst_author_username.append(i['author']['username'])\n",
    "        lst_twt_creation_time.append(i['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8768c389-a75e-45af-be8b-cf8940199c75",
   "metadata": {},
   "source": [
    " #### Put the author info in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad13fe-db12-4fe4-a859-87dd1586bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a61f6d-633b-4c1d-a9ea-0599dfeb3afd",
   "metadata": {},
   "source": [
    "##### Put what we have in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0e8f52-6dfe-4a2b-bbb4-c5ec3fefb98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame({'text': lst_o_text,\n",
    "                  'language': lst_lang,\n",
    "                  'id': lst_id,\n",
    "                  'author_id': lst_author_id,\n",
    "                  'conversation_id': lst_conversation_id,\n",
    "                  'author_verified': lst_author_verified,\n",
    "                  'author_name': lst_author_name,\n",
    "                  'author_username': lst_author_username,\n",
    "                  'twt_creation_time': lst_twt_creation_time\n",
    "                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188315ed-e472-403c-9860-0cd9bebccca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c9a2a-cb70-427a-900a-44f7c76611c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.drop(tweets[tweets['language'] != 'en'].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe06b4c-50a9-47c3-a3bc-b9802eaa7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f8239d-0094-46e7-ab51-c559cb9963fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae12cec-2bd4-43dc-bdd8-e442a6907d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['hashtag'] = tweets['text'].apply(lambda x: re.findall(r\"#(\\w+)\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4bb11c-5b98-452c-8ea9-e1f0f4e36ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forming a separate column for cleaned tweets\n",
    "tweets['ctext'] = tweets['text'].apply(lambda x: p.clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e878d-4990-4b58-85f2-2e42650a652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef3a4db-2aa1-412a-b4b6-6a07696d38d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to list\n",
    "ctext = tweets['ctext'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f210384d-bd53-4a61-b085-e7e1545ee33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nctext = tweets['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ad07de-a943-4a3d-a64e-0a555ecc16d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove new line characters\n",
    "ctext = [re.sub('\\s+', ' ', sent) for sent in ctext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10de78ad-cd53-4a1e-b4d8-42cf0466e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove distracting single quotes\n",
    "ctext = [re.sub(\"\\'\", \"\", sent) for sent in ctext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b12d3-e30d-437a-82d6-8ff049bc9a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "ctext = [re.sub(r'[^\\w\\s]', '', sent) for sent in ctext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48ea990-81ac-4293-8fc2-cad5bc777db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctext[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee9d1b7-d916-49e8-9467-c99192da368b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tokenize words and further clean up text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa60cee8-1c55-469f-ae2f-ed218555bee5",
   "metadata": {},
   "source": [
    "Tokenize each sentence into a list of words, removing punctuations and unnecessary characters altoghter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b44245f-17f9-45d5-8ebd-bc0c18503060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "tweet_words = list(sent_to_words(ctext))\n",
    "\n",
    "print(tweet_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef03c9-848f-4004-824e-b2340c6086f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(tweet_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[tweet_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[tweet_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a6586-a208-47dc-b7df-e84b8c5a621a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Remove Stopwords, Make Bigrams and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b527df8e-7e82-45eb-995f-64d60d7d5dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9ae696-61af-40ea-8380-ea5b22393074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91edbe4c-1e69-4d5c-b09a-318a5c3047d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f1398-6c05-4437-9052-fdc9f25af0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "tweet_words_nostops = remove_stopwords(tweet_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c6dc85-e89e-4ff4-a4d1-83fede7b1a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "tweet_words_bigrams = make_bigrams(tweet_words_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc2cc77-8a72-423b-ae6c-6546c8ade7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy 'en_core_web_sm' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])# not \"en\" any more\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "tweet_lemmatized = lemmatization(tweet_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(tweet_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b9a70-7bf6-4b5b-8391-76185dd7cea7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creat the dictionary and the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045af5c1-03ce-4c46-a4ce-5d2510470d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(tweet_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = tweet_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts] \n",
    "\n",
    "#Gensim creates a unique id for each word in the document. \n",
    "#The produced corpus shown below is a mapping of (word_id, word_frequency(times of occurance)).\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c70fb-e5f5-497a-89f3-9892a245c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ed933-af4b-42eb-a1f3-d6e64196b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to see what word a given id corresponds to, pass the id as a key to the dictionary.\n",
    "id2word[4]\n",
    "\n",
    "#input of LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520aec4-4caa-4db2-bcad-f2eea66c6b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82855c8f-5efb-4c9d-872f-6a1ac7954577",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Building the Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2479a5-4314-45dd-98d5-a06ef929f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=200,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c1416a-9b9f-49b1-95b7-4105946d733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a08cd6-7d24-4d44-b4b9-398195a86847",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute model perplexity and coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d82ce0a-a3f3-410d-84e0-140df9173b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=tweet_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77942d7d-c8f8-4bd2-9df3-2fd7cdbb7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e69ee4-15d9-47c5-844d-117d707ab222",
   "metadata": {
    "tags": []
   },
   "source": [
    "# The Following code is still being working on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f4e9e-9158-4afc-933d-8571b63db883",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Find the optimal number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad00526-2503-4f3f-9f45-cbf49d3eb72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3900923d-71e3-48cc-9874-a5c45678b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# May take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=tweet_lemmatized, start=2, limit=20, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3415d926-fdd9-4d0e-8d5e-7620b147d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=20; start=2; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa752331-e3ad-4d55-8b56-b4b5b2be33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457081c-f856-4de7-854e-8b8bfca718ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "optimal_model = model_list[3]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89277f1f-5465-446a-b195-baa74dab95e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the optimal topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(optimal_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a55ce8-e7e1-49a3-b75a-d0274de33505",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Finding the dominant topic in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78407f-fd9f-4051-afaf-9ae1e492d546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=ctext):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=nctext)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac667693-c3cf-41c6-b3f7-f20dc92ffd93",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Find the most representative document for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af1d4e-6496-4096-817a-52ea6e513e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59576a82-5edc-41c6-ab6e-1cd2a5935440",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Topic distribution across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1695dc0-dc55-4ed8-8636-14b13e5cfb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45614d3e-d5fc-4bc4-a25f-61ab57f845a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
